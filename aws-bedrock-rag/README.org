* AWS Bedrock RAG Tutorial
:PROPERTIES:
:CUSTOM_ID: aws-bedrock-rag
:END:

A hands-on tutorial implementing Retrieval Augmented Generation (RAG) with AWS Bedrock, demonstrating real-world application of AWS AI/ML services. This is part of the AWS AI Foundation certification (C01) preparation materials.

*Duration:* 4 hours

** Overview
:PROPERTIES:
:CUSTOM_ID: overview
:END:

This tutorial demonstrates practical implementation of RAG systems on AWS, featuring:

- Multiple retrieval strategies:
  - Basic semantic search using Voyage AI embeddings
  - BM25 text search using Elasticsearch
  - Contextual embeddings with Claude-powered document understanding
  - Hybrid retrieval combining semantic and lexical search
- Comprehensive evaluation framework
- GitHub repository integration
- AWS Bedrock knowledge base integration

This implementation aligns with AWS AI Foundation certification exam objectives covering:
- AWS Bedrock capabilities and integration
- Machine learning implementation patterns
- AI application architecture best practices

** Architecture
:PROPERTIES:
:CUSTOM_ID: architecture
:END:

#+begin_src mermaid :file architecture.png :mkdirp t
flowchart TB
    subgraph DataIngestion["Data Ingestion Layer"]
        G[GitHub Repos] --> C[Crawler]
        C --> P[Preprocessor]
        P --> |Chunks| V[Vector Storage]
    end

    subgraph RetrievalPipeline["Retrieval Pipeline"]
        subgraph VectorSearch["Vector Search"]
            V --> S1[Semantic Search]
            V --> S2[BM25 Search]
        end
        
        subgraph Enrichment["Enrichment"]
            S1 --> H[Hybrid Retriever]
            S2 --> H
            H --> R[Reranker]
        end
    end
    
    subgraph KnowledgeBase["AWS Bedrock Integration"]
        V --> B[Bedrock KB]
        B --> L[Language Models]
    end
    
    subgraph Evaluation["Evaluation Framework"]
        M1[Metrics Collector]
        M2[Performance Analysis]
        M3[Comparison Tools]
    end

    P --> B
    R --> E{Embeddings}
    E --> |Voyage AI| S1
    E --> |Claude Context| S2
    R --> L
    R --> M1
    M1 --> M2
    M2 --> M3

    classDef aws fill:#FF9900,stroke:#232F3E,stroke-width:2px,color:white
    classDef external fill:#3B82F6,stroke:#1E40AF,stroke-width:2px,color:white
    classDef pipeline fill:#10B981,stroke:#065F46,stroke-width:2px,color:white
    classDef eval fill:#8B5CF6,stroke:#5B21B6,stroke-width:2px,color:white
    
    class B,L aws
    class G,E external
    class S1,S2,H,R pipeline
    class M1,M2,M3 eval
#+end_src

The system consists of several key components demonstrating AWS AI service integration:

*** Data Ingestion
   - GitHub repository crawler (=retrieve_github_data.py=)
   - Document chunking and preprocessing
   - AWS Bedrock knowledge base creation

*** Retrieval Pipeline
   - Vector database with Voyage AI embeddings (=vector_db.py=)
   - Contextual vector database (=contextual_vector_db.py=)
   - BM25 search integration (=contextual_bm25.py=)
   - Re-ranking system (=reranking.py=)

*** Evaluation Framework
   - Multiple evaluation metrics
   - Comparison across different retrieval methods
   - Performance analysis tools

** Prerequisites
:PROPERTIES:
:CUSTOM_ID: prerequisites
:END:

*** Required Software
- Python 3.8+
- AWS Account with Bedrock access
- =boto3= library installed (=pip install boto3=)

*** Required Libraries
- =anthropic= - to interact with Claude
- =voyageai= - to generate high quality embeddings
- =pandas=, =numpy=, =matplotlib=, and =scikit-learn= for data manipulation
- =elasticsearch= for BM25 search

*** API Keys
- GitHub Personal Access Token
- Anthropic API Key
- Voyage AI API Key
- Cohere API Key (for reranking)

** Getting Started
:PROPERTIES:
:CUSTOM_ID: getting-started
:END:

*** Environment Setup
#+begin_src bash
# From project root
cd aws-bedrock-rag

# Set up environment variables
export GITHUB_TOKEN="your_github_token"
export ANTHROPIC_API_KEY="your_anthropic_key"
export VOYAGE_API_KEY="your_voyage_key"
export COHERE_API_KEY="your_cohere_key"
#+end_src

*** Initial Dependencies
#+begin_src python :tangle setup.py
!pip install anthropic voyageai pandas numpy matplotlib seaborn scikit-learn

import os
os.environ['VOYAGE_API_KEY'] = "VOYAGE KEY HERE"
os.environ['ANTHROPIC_API_KEY'] = "ANTHROPIC KEY HERE"

import anthropic
client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
#+end_src

*** Data Collection
#+begin_src bash
# Fetch repository data
python retrieve_github_data.py

# Process and chunk documents
python load_data.py
#+end_src

*** Vector Database Initialization
#+begin_src bash
# Basic vector database
python load_data.py

# Contextual vector database
python load_contextual_data.py
#+end_src

*** Evaluation Pipeline
#+begin_src bash
# Basic RAG evaluation
python evaluate_basic_rag.py

# Contextual embeddings evaluation
python evaluate_contextual_embeddings.py

# BM25 hybrid evaluation
python evaluate_contextual_bm25.py

# Reranking evaluation
python evaluate_reranking.py
#+end_src

** Implementation Details
:PROPERTIES:
:CUSTOM_ID: implementation-details
:END:

*** GitHub Data Retrieval
#+begin_src python :tangle retrieve_github_data.py
import os
import requests

GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')

def get_org_repos(org_name):
    headers = {'Authorization': f'token {GITHUB_TOKEN}'}
    url = f'https://api.github.com/orgs/{org_name}/repos'
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.json()
#+end_src

*** AWS Bedrock Integration
#+begin_src python :tangle bedrock_integration.py
import boto3

bedrock = boto3.client('bedrock')

def create_knowledge_base(kb_name, data_source):
    # Implement logic to create a knowledge base in Bedrock
    # using the provided data_source (from GitHub repo data)
    pass
#+end_src

*** Contextual Embeddings
#+begin_src python
def situate_context(doc: str, chunk: str) -> str:
    # Generate context-aware embeddings using Claude
    response = client.beta.prompt_caching.messages.create(
        model="claude-3-haiku-20240307",
        messages=[...],
        temperature=0.0
    )
    return response
#+end_src

*** Hybrid Retrieval
#+begin_src python
def retrieve_advanced(query: str, db, es_bm25, k: int):
    # Semantic search
    semantic_results = db.search(query, k=k)
    
    # BM25 search
    bm25_results = es_bm25.search(query, k=k)
    
    # Combine and rerank results
    return combine_results(semantic_results, bm25_results)
#+end_src

** Performance Metrics
:PROPERTIES:
:CUSTOM_ID: performance-metrics
:END:

Through our improvements, we achieved significant gains:

| Metric                       | Before | After |
|-----------------------------+--------+-------|
| Avg Precision               |   0.43 |  0.46 |
| Avg Recall                  |   0.66 |  0.74 |
| Avg F1 Score                |   0.52 |  0.57 |
| Avg Mean Reciprocal Rank    |   0.74 |  0.93 |
| End-to-End Accuracy         |    70% |   83% |

Additional metrics tracked:
- Pass@k (k=5,10,20)
- Semantic vs. BM25 contribution analysis
- Cache hit rates for contextual embeddings

** Related Resources
:PROPERTIES:
:CUSTOM_ID: related-resources
:END:

- [[file:../aws-services-overview.org][AWS Services Overview]]
- [[https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html][AWS Bedrock Documentation]]
- [[https://github.com/anthropics/anthropic-cookbook/blob/main/skills/retrieval_augmented_generation/guide.ipynb][Anthropic Cookbook - RAG Guide]]
- [[https://github.com/anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide.ipynb][Anthropic Cookbook - Contextual Embeddings]]

** Notes
:PROPERTIES:
:CUSTOM_ID: notes
:END:

*** Implementation Notes
- Using in-memory vector DB for examples (consider hosted solutions for production)
- Evaluations mirror production systems and can be time-intensive
- Consider rate limits if not on Tier 2 or above
- May want to skip full end-to-end eval to conserve token usage

*** Certification Topics Covered
- Foundation model integration with AWS Bedrock
- Vector database implementation patterns
- Hybrid search architectures
- Performance evaluation methodologies

*** Study Focus Areas
- Study the AWS Bedrock configuration in =bedrock_integration.py=
- Understand the retrieval pipeline architecture
- Review the evaluation metrics and their significance
- Consider how this implementation maps to exam objectives
